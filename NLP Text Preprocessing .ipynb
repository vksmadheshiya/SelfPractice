{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNOFsqGHXSZzsgLXsCV58vR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xybkxPQcUtTD","executionInfo":{"status":"ok","timestamp":1689843100020,"user_tz":-330,"elapsed":5,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"0197c716-a849-4f65-d462-4056db6be48b"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"]}],"source":["import nltk\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","\n","import string\n","import re"]},{"cell_type":"code","source":["def text_lowercase(text):\n","\treturn text.lower()\n","\n","input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n","text_lowercase(input_str)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"roi0CLY-U5ZA","executionInfo":{"status":"ok","timestamp":1689840245406,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"cf117951-f2fc-4007-f15d-48fe6d767f81"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"hey, did you know that the summer break is coming? amazing right !! it's only 5 more days !!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["Remove numbers usi9ng re:\n"],"metadata":{"id":"3jcfmhOUVAsW"}},{"cell_type":"code","source":["# Remove numbers\n","def remove_numbers(text):\n","\tresult = re.sub(r'\\d+', '', text)\n","\treturn result\n","\n","input_str = \"There are 3 balls in this bag, and 12 in the other one.\"\n","remove_numbers(input_str)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"XLNhjyFCU9dN","executionInfo":{"status":"ok","timestamp":1689840279481,"user_tz":-330,"elapsed":7,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"b49f625f-2b45-4173-ac84-d281399bc4d1"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'There are  balls in this bag, and  in the other one.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["convert the numbers into words"],"metadata":{"id":"tLaozEu5VJat"}},{"cell_type":"code","source":["# import the inflect library\n","import inflect\n","p = inflect.engine()"],"metadata":{"id":"Hb4yPQ6eVFuj","executionInfo":{"status":"ok","timestamp":1689840326511,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# convert number into words\n","def convert_number(text):\n","\t# split string into list of words\n","\ttemp_str = text.split()\n","\t# initialise empty list\n","\tnew_string = []\n","\n","\tfor word in temp_str:\n","\t\t# if word is a digit, convert the digit\n","\t\t# to numbers and append into the new_string list\n","\t\tif word.isdigit():\n","\t\t\ttemp = p.number_to_words(word)\n","\t\t\tnew_string.append(temp)\n","\n","\t\t# append the word as it is\n","\t\telse:\n","\t\t\tnew_string.append(word)\n","\n","\t# join the words of new_string to form a string\n","\ttemp_str = ' '.join(new_string)\n","\treturn temp_str\n","\n","input_str = 'There are 3 balls in this bag, and 12 in the other one.'\n","convert_number(input_str)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"WRYLoz26VOZn","executionInfo":{"status":"ok","timestamp":1689840358335,"user_tz":-330,"elapsed":6,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"eb8608c8-0fba-4ced-97f5-201b8498e08b"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'There are three balls in this bag, and twelve in the other one.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["Remove punctuation:"],"metadata":{"id":"JgahHUe2Vf02"}},{"cell_type":"code","source":["\n","# remove punctuation\n","def remove_punctuation(text):\n","  translator = str.maketrans('', '', string.punctuation)\n","  return text.translate(translator)\n","\n","input_str = \"Hey, did you know that the summer break is coming? Amazing right !! It's only 5 more days !!\"\n","remove_punctuation(input_str)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"tV8aNdw7VY3n","executionInfo":{"status":"ok","timestamp":1689840508719,"user_tz":-330,"elapsed":13,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"fa4c553b-76bd-40a2-aac4-2b4675448e8a"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Hey did you know that the summer break is coming Amazing right  Its only 5 more days '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["Remove whitespaces:\n"],"metadata":{"id":"Ee5G42b6XIol"}},{"cell_type":"code","source":["# remove whitespace from text\n","def remove_whitespace(text):\n","\treturn \" \".join(text.split())\n","\n","input_str = \" we don't need the given questions\"\n","remove_whitespace(input_str)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"kmkl1vXnV9s1","executionInfo":{"status":"ok","timestamp":1689840824878,"user_tz":-330,"elapsed":14,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"c6105d83-9013-433b-dd85-9f945a6a9e84"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"we don't need the given questions\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["## Remove default stopwords:\n","Stopwords are words that do not contribute to the meaning of a sentence."],"metadata":{"id":"OzouUcewXOIs"}},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize"],"metadata":{"id":"fB3ywh3qXK7w","executionInfo":{"status":"ok","timestamp":1689840918905,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["stop_words = set(stopwords.words(\"english\"))\n","print(stop_words)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z04P6C1MYIMo","executionInfo":{"status":"ok","timestamp":1689841161211,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"f52ca074-8bdf-4a29-c217-f7b7d8e6b204"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["{'isn', 'for', 'very', 'into', 'them', 'being', 'won', \"hasn't\", 'hasn', 'same', 'some', 'needn', \"weren't\", 'up', 'i', 'weren', 'she', \"isn't\", 'not', 'once', 'who', 'nor', 'had', 'ain', \"shouldn't\", 'mightn', \"she's\", 'hadn', 'there', 'ours', 'how', \"wouldn't\", 'all', 'we', \"haven't\", 'such', 'your', 'own', 'only', 'so', 'yourself', 'before', 'most', \"mustn't\", 'down', 'whom', 'too', 'do', 'which', 'should', 'his', 'd', 'wouldn', 'and', 'doing', 'further', 'didn', \"should've\", 'that', 'my', 'you', 'haven', 'any', 'their', 'each', \"shan't\", \"don't\", \"wasn't\", 'be', \"aren't\", 'why', 'more', 'are', 'in', 'what', 'can', 'its', 'did', \"won't\", 'off', \"it's\", 'by', 'yourselves', 'm', 'y', 'does', 'mustn', 'while', 'don', 'have', 'then', 'theirs', \"you'll\", 'is', 'now', 'been', \"that'll\", 'on', 't', \"hadn't\", 'these', 'ourselves', 'him', 'over', \"you'd\", 'they', \"needn't\", 'out', 'me', 'myself', 'than', \"you're\", 'it', 'hers', 'as', 'ma', 've', 'will', \"doesn't\", \"you've\", 'below', 'll', 'shan', 'an', 'between', 'again', 'he', 'where', \"mightn't\", 'of', 'her', 'has', 'our', 'here', 'couldn', \"didn't\", 'shouldn', 're', 'themselves', 'yours', 'at', 'against', 'because', 'aren', 'a', 'about', 'during', 'under', 'wasn', 'but', 'himself', 'were', 'through', 'above', 'this', 'those', 'was', 'having', 'both', 'other', 'until', 'to', 'the', 'after', 'from', 'with', 'when', 's', 'herself', 'doesn', 'no', 'o', 'itself', 'just', \"couldn't\", 'am', 'if', 'few', 'or'}\n"]}]},{"cell_type":"code","source":["# remove stopwords function\n","def remove_stopwords(text):\n","  stop_words = set(stopwords.words(\"english\"))\n","  word_tokens = word_tokenize(text)\n","  filtered_text = [word for word in word_tokens if word not in stop_words]\n","  return filtered_text\n","\n","example_text = \"This is a sample sentence and we are going to remove the stopwords from this.\"\n","remove_stopwords(example_text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Abp-w0TMXhtB","executionInfo":{"status":"ok","timestamp":1689841224552,"user_tz":-330,"elapsed":9,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"57b769ad-628b-475f-b029-fc228a150b51"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['This', 'sample', 'sentence', 'going', 'remove', 'stopwords', '.']"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["**Stemming:**\n","Stemming is the process of getting the root form of a word. Stem or root is the part to which inflectional affixes (-ed, -ize, -de, -s, etc.) are added. The stem of a word is created by removing the prefix or suffix of a word. So, stemming a word may not result in actual words."],"metadata":{"id":"DuT9eQ22YvKB"}},{"cell_type":"code","source":["\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","stemmer = PorterStemmer()\n","\n","# stem words in the list of tokenized words\n","def stem_words(text):\n","  word_tokens = word_tokenize(text)\n","  stems = [stemmer.stem(word) for word in word_tokens]\n","  return stems\n","\n","text = 'data science uses scientific methods algorithms and many types of processes'\n","stem_words(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnND7-A7YTpt","executionInfo":{"status":"ok","timestamp":1689841456349,"user_tz":-330,"elapsed":3,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"313a290f-867c-4ea1-c6b1-0b24655e77f1"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['data',\n"," 'scienc',\n"," 'use',\n"," 'scientif',\n"," 'method',\n"," 'algorithm',\n"," 'and',\n"," 'mani',\n"," 'type',\n"," 'of',\n"," 'process']"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["**Lemmatization:**\n","Like stemming, lemmatization also converts a word to its root form. The only difference is that lemmatization ensures that the root word belongs to the language. We will get valid words if we use lemmatization. In NLTK, we use the WordNetLemmatizer to get the lemmas of words. We also need to provide a context for the lemmatization. So, we add the part-of-speech as a parameter.\n"],"metadata":{"id":"-eokDFYoZpqp"}},{"cell_type":"code","source":["from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","lemmatizer = WordNetLemmatizer()\n","# lemmatize string\n","def lemmatize_word(text):\n","\tword_tokens = word_tokenize(text)\n","\t# provide context i.e. part-of-speech\n","\tlemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n","\treturn lemmas\n","\n","text = 'data science uses scientific methods algorithms and many types of processes'\n","lemmatize_word(text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbkwCGF2ZlBt","executionInfo":{"status":"ok","timestamp":1689841792190,"user_tz":-330,"elapsed":1547,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"7162ab2d-d49f-41b7-f7b4-14873711b984"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['data',\n"," 'science',\n"," 'use',\n"," 'scientific',\n"," 'methods',\n"," 'algorithms',\n"," 'and',\n"," 'many',\n"," 'type',\n"," 'of',\n"," 'process']"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["Part of Speech Tagging:\n"],"metadata":{"id":"K7_SnsNka-VE"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","\n","# convert text into word_tokens with their tags\n","def pos_tagging(text):\n","    word_tokens = word_tokenize(text)\n","    return pos_tag(word_tokens)\n","\n","pos_tagging('You just gave me a scare')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qp5sGpJpaybi","executionInfo":{"status":"ok","timestamp":1689841976454,"user_tz":-330,"elapsed":549,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"e376ec94-801f-4f50-cd4c-69f6a35d29a7"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('You', 'PRP'),\n"," ('just', 'RB'),\n"," ('gave', 'VBD'),\n"," ('me', 'PRP'),\n"," ('a', 'DT'),\n"," ('scare', 'NN')]"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# download the tagset\n","nltk.download('tagsets')\n","\n","# extract information about the tag\n","nltk.help.upenn_tagset('NN')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H2hlexkobgd9","executionInfo":{"status":"ok","timestamp":1689842047329,"user_tz":-330,"elapsed":2,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"68facdfa-90b6-47e7-b651-13b0d5fe9947"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package tagsets to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["NN: noun, common, singular or mass\n","    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n","    investment slide humour falloff slick wind hyena override subhumanity\n","    machinist ...\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data]   Unzipping help/tagsets.zip.\n"]}]},{"cell_type":"markdown","source":["**Chunking:**\n","Chunking is the process of extracting phrases from unstructured text and more structure to it. It is also known as shallow parsing. It is done on top of Part of Speech tagging. It groups word into “chunks”, mainly of noun phrases. Chunking is done using regular expressions.\n","\n","\n","Libraries like spaCy and Textblob are more suited for chunking.\n","\n"],"metadata":{"id":"VRfIcrswb65x"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk import pos_tag\n","\n","# define chunking function with text and regular\n","# expression representing grammar as parameter\n","def chunking(text, grammar):\n","  word_tokens = word_tokenize(text)\n","\n","  # label words with part of speech\n","  word_pos = pos_tag(word_tokens)\n","\n","  # create a chunk parser using grammar\n","  chunkParser = nltk.RegexpParser(grammar)\n","\n","  # test it on the list of word tokens with tagged pos\n","  tree = chunkParser.parse(word_pos)\n","\n","  for subtree in tree.subtrees():\n","    print(subtree)\n","  # tree.draw()\n","\n","\n","sentence = 'the little yellow bird is flying in the sky'\n","grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n","chunking(sentence, grammar)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GEy56t4hb1Q1","executionInfo":{"status":"ok","timestamp":1689842352015,"user_tz":-330,"elapsed":600,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"4e8b2042-1360-483b-fb6f-4df883948ae6"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (NP the/DT little/JJ yellow/JJ bird/NN)\n","  is/VBZ\n","  flying/VBG\n","  in/IN\n","  (NP the/DT sky/NN))\n","(NP the/DT little/JJ yellow/JJ bird/NN)\n","(NP the/DT sky/NN)\n"]}]},{"cell_type":"markdown","source":["In the given example, grammar, which is defined using a simple regular expression rule. This rule says that an NP (Noun Phrase) chunk should be formed whenever the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and then a noun (NN)."],"metadata":{"id":"_Js9JMfqdlSP"}},{"cell_type":"markdown","source":["## Named Entity Recognition:\n","\n","Named Entity Recognition is used to extract information from unstructured text. It is used to classify entities present in a text into categories like a person, organization, event, places, etc. It gives us detailed knowledge about the text and the relationships between the different entities."],"metadata":{"id":"OeV8mTl2e565"}},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","from nltk import pos_tag, ne_chunk\n","\n","def named_entity_recognition(text):\n","  # tokenize the text\n","  word_tokens = word_tokenize(text)\n","\n","  # part of speech tagging of words\n","  word_pos = pos_tag(word_tokens)\n","\n","  # tree of word entities\n","  print(ne_chunk(word_pos))\n","\n","text = 'Bill works for XYZ so he went to Delhi for a meetup.'\n","named_entity_recognition(text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1xCe0zQc65d","executionInfo":{"status":"ok","timestamp":1689843107265,"user_tz":-330,"elapsed":408,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"f99e3465-f171-42be-d05d-f40253784522"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["(S\n","  (PERSON Bill/NNP)\n","  works/VBZ\n","  for/IN\n","  (ORGANIZATION XYZ/NNP)\n","  so/RB\n","  he/PRP\n","  went/VBD\n","  to/TO\n","  (GPE Delhi/NNP)\n","  for/IN\n","  a/DT\n","  meetup/NN\n","  ./.)\n"]}]},{"cell_type":"markdown","source":["Removing stop words with NLTK\n"],"metadata":{"id":"MtrLmVCsgtFd"}},{"cell_type":"code","source":["\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","example_sent = \"\"\"This is a sample sentence,\n","                  showing off the stop words filtration.\"\"\"\n","\n","stop_words = set(stopwords.words('english'))\n","\n","word_tokens = word_tokenize(example_sent)\n","# converts the words in word_tokens to lower case and then checks whether\n","#they are present in stop_words or not\n","\n","filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n","print(filtered_sentence)\n","#with no lower case conversion\n","filtered_sentence = []\n","\n","#with no lower case conversion\n","filtered_sentence = []\n","\n","for w in word_tokens:\n","    if w not in stop_words:\n","        filtered_sentence.append(w)\n","\n","print(word_tokens)\n","print(filtered_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMgu5ghpfslA","executionInfo":{"status":"ok","timestamp":1689843572809,"user_tz":-330,"elapsed":410,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"98159f72-cc33-4aae-f854-efec7ea8da11"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n","['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n","['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"]}]},{"cell_type":"markdown","source":["Performing the Stopwords operations in a file\n"],"metadata":{"id":"Y0l5emSZh0Ec"}},{"cell_type":"code","source":["import io\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# word_tokenize accepts\n","# a string as an input, not a file.\n","stop_words = set(stopwords.words('english'))\n","file1 = open(\"text.txt\")\n","\n","# Use this to read file content as a stream:\n","line = file1.read()\n","words = line.split()\n","for r in words:\n","    if not r in stop_words:\n","        appendFile = open('filteredtext.txt','a')\n","        appendFile.write(\" \"+r)\n","        appendFile.close()"],"metadata":{"id":"Ik0xd5o9hppl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Sentence and Word Tokenization"],"metadata":{"id":"-7hDJUAfjVoW"}},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize , word_tokenize\n","\n","text = \"Natural language processing (NLP) is a field \" + \\\n","       \"of computer science, artificial intelligence \" + \\\n","       \"and computational linguistics concerned with \" + \\\n","       \"the interactions between computers and human \" + \\\n","       \"(natural) languages, and, in particular, \" + \\\n","       \"concerned with programming computers to \" + \\\n","       \"fruitfully process large natural language \" + \\\n","       \"corpora. Challenges in natural language \" + \\\n","       \"processing frequently involve natural \" + \\\n","       \"language understanding, natural language\" + \\\n","       \"generation frequently from formal, machine\" + \\\n","       \"-readable logical forms), connecting language \" + \\\n","       \"and machine perception, managing human-\" + \\\n","       \"computer dialog systems, or some combination \" + \\\n","       \"thereof.\"\n","\n","print(sent_tokenize(text))\n","print(word_tokenize(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S0O2CMoFjUU4","executionInfo":{"status":"ok","timestamp":1689844074605,"user_tz":-330,"elapsed":410,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"5b115110-c472-462d-e4bd-1016e528f326"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.']\n","['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.']\n"]}]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize\n","\n","text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\"\n","sent_tokenize(text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cEpgKbCQjkKO","executionInfo":{"status":"ok","timestamp":1689844193672,"user_tz":-330,"elapsed":452,"user":{"displayName":"Vikas Madheshiya","userId":"13316776056308098169"}},"outputId":"ce6c467e-5cb7-4033-de09-73dbf8f1cb4c"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Hello everyone.',\n"," 'Welcome to GeeksforGeeks.',\n"," 'You are studying NLP article']"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":[],"metadata":{"id":"89Wj_PHRkBTk"},"execution_count":null,"outputs":[]}]}